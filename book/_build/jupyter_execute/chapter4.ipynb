{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGpBWliOQc5R"
   },
   "source": [
    "## **Chapter 4 ‚Äì Large Language Models in Chemistry**\n",
    "\n",
    "\n",
    "This chapter explores how transformer-based LLMs can be applied in the chemical sciences. We cover how they process chemical information, strategies for prompting and fine-tuning, and building chemistry-focused conversational agents. Extensions to multimodal data (structures, spectra) and integration with external tools are discussed, along with evaluation, reliability, and safety (e.g., avoiding hallucinations). By the end, readers will understand how to apply and adapt LLMs to chemical problems responsibly.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HBe1WlpV0qXc"
   },
   "source": [
    "Step 1: Prepare Your Dataset  \n",
    "\n",
    "Gather examples of the task you want the model to learn (e.g., question‚Äìanswer or prompt‚Äìcompletion pairs).  \n",
    "For instance, if you want a model that assists with chemical reaction questions, you could collect problems as **prompts** and their solutions as **completions**.  \n",
    "\n",
    "The dataset must be in **JSONL (JSON Lines)** format, where each line is a JSON object with `\"prompt\"` and `\"completion\"` fields. The `\"prompt\"` is the input you would provide at runtime, and the `\"completion\"` is the expected response.  \n",
    "\n",
    "Be sure to include any formatting in the prompt that you plan to use during inference.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dYK-0PDF0ltj",
    "outputId": "942d06ea-1c74-4013-ee60-d2a798324bdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote chat-format file!\n",
      "{\"messages\": [{\"role\": \"system\", \"content\": \"You are a chemistry tutor.\"}, {\"role\": \"user\", \"content\": \"What is the formula of acetone?\"}, {\"role\": \"assistant\", \"content\": \"C3H6O\"}]}\n",
      "{\"messages\": [{\"role\": \"system\", \"content\": \"You are a chemistry tutor.\"}, {\"role\": \"user\", \"content\": \"How many 1H NMR signals would benzene have?\"}, {\"role\": \"assistant\", \"content\": \"1 (all hydrogens are equivalent)\"}]}\n",
      "{\"messages\": [{\"role\": \"system\", \"content\": \"You are a chemistry tutor.\"}, {\"role\": \"user\", \"content\": \"What functional group is present in ethanol?\"}, {\"role\": \"assistant\", \"content\": \"A hydroxyl (-OH) group\"}]}\n",
      "{\"messages\": [{\"role\": \"system\", \"content\": \"You are a chemistry tutor.\"}, {\"role\": \"user\", \"content\": \"What is the pKa of acetic acid approximately?\"}, {\"role\": \"assistant\", \"content\": \"Around 4.76\"}]}\n",
      "{\"messages\": [{\"role\": \"system\", \"content\": \"You are a chemistry tutor.\"}, {\"role\": \"user\", \"content\": \"What is the molecular geometry of methane?\"}, {\"role\": \"assistant\", \"content\": \"Tetrahedral\"}]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "data = [\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a chemistry tutor.\"},\n",
    "            {\"role\": \"user\", \"content\": \"What is the formula of acetone?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"C3H6O\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a chemistry tutor.\"},\n",
    "            {\"role\": \"user\", \"content\": \"How many 1H NMR signals would benzene have?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"1 (all hydrogens are equivalent)\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a chemistry tutor.\"},\n",
    "            {\"role\": \"user\", \"content\": \"What functional group is present in ethanol?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"A hydroxyl (-OH) group\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a chemistry tutor.\"},\n",
    "            {\"role\": \"user\", \"content\": \"What is the pKa of acetic acid approximately?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Around 4.76\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a chemistry tutor.\"},\n",
    "            {\"role\": \"user\", \"content\": \"What is the molecular geometry of methane?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Tetrahedral\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a chemistry tutor.\"},\n",
    "            {\"role\": \"user\", \"content\": \"What hybridization does the carbon atom in ethyne (C2H2) have?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"sp hybridization\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a chemistry tutor.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Which reagent is commonly used to test for unsaturation (double bonds) in hydrocarbons?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Bromine water (Br2 in CCl4)\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a chemistry tutor.\"},\n",
    "            {\"role\": \"user\", \"content\": \"What type of bond is formed between Na+ and Cl- in NaCl?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"An ionic bond\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a chemistry tutor.\"},\n",
    "            {\"role\": \"user\", \"content\": \"What is the IUPAC name of CH3-CH2-CH2-OH?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Propan-1-ol\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a chemistry tutor.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Which gas is released when hydrochloric acid reacts with sodium carbonate?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Carbon dioxide (CO2)\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "with open(\"chem_tuning_data_chat.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for row in data:\n",
    "        f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Wrote chat-format file!\")\n",
    "!head -n 5 chem_tuning_data_chat.jsonl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7OSI1Ldu1T7n"
   },
   "source": [
    "Step 2: Upload and Validate the Dataset  \n",
    "\n",
    "Once you have a `.jsonl` file (e.g., `chem_tuning_data.jsonl`), you can use the **OpenAI CLI** to validate and upload it. OpenAI provides a handy command to check the format:  \n",
    "\n",
    "```bash\n",
    "openai api files.create -f chem_tuning_data_chat.jsonl -p fine-tune\n",
    "```\n",
    "To run this successfully, you must have your API key set as OPENAI_API_KEY in your environment.\n",
    "\n",
    "In Colab, you can use the **Secrets tab** (üîë) to securely store your API key.  \n",
    "The CLI and Python libraries will automatically detect it if set as `OPENAI_API_KEY`.  \n",
    "\n",
    "In general, the steps to get started are:  \n",
    "1. Create an **OpenAI account**  \n",
    "2. Generate an **API key** from your account dashboard  \n",
    "3. Set the key as an environment variable (`OPENAI_API_KEY`) in your working environment  \n",
    "\n",
    "You can set it in different ways:  \n",
    "```python\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-your-api-key\"\n",
    "```\n",
    "or\n",
    "```bash\n",
    "export OPENAI_API_KEY=\"sk-your-api-key\"\n",
    "```\n",
    "‚ö†Ô∏è Important: API keys grant access to your OpenAI account. Never share them in public notebooks, commits, or screenshots. Use environment variables or Colab‚Äôs secret manager to keep them secure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WBdEPIbVT9Wp"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A3soYt4u1ibQ",
    "outputId": "87bec8bd-ca2c-44ee-c698-e80d12e998de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload progress: 100% 2.16k/2.16k [00:00<00:00, 3.62kit/s]\n",
      "{\n",
      "  \"id\": \"file-FFgRh4gh7pBug9Lsrb7m6e\",\n",
      "  \"bytes\": 2163,\n",
      "  \"created_at\": 1759130284,\n",
      "  \"filename\": \"chem_tuning_data_chat.jsonl\",\n",
      "  \"object\": \"file\",\n",
      "  \"purpose\": \"fine-tune\",\n",
      "  \"status\": \"processed\",\n",
      "  \"expires_at\": null,\n",
      "  \"status_details\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!openai api files.create -f chem_tuning_data_chat.jsonl -p fine-tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a12HNJlH1l-9"
   },
   "source": [
    "### Step 3: Initiate the Fine-Tuning Job  \n",
    "\n",
    "Now you can start the fine-tuning process by specifying a base model.  \n",
    "As of 2025, OpenAI supports fine-tuning on certain GPT-3.5 and GPT-4 variants (e.g., `gpt-3.5-turbo` and a special `gpt-4o` model).  \n",
    "\n",
    "For example, to fine-tune `gpt-3.5-turbo` with your uploaded file:  \n",
    "\n",
    "```bash\n",
    "openai api fine_tunes.create -t \"<YOUR FILE ID>\" -m \"gpt-3.5-turbo\"\n",
    "```\n",
    "Replace <YOUR FILE ID> with the ID you received from the upload step (it typically starts with file-).\n",
    "\n",
    "This command kicks off the fine-tuning job. By default, OpenAI sets sensible training hyperparameters (epochs, learning rate). You may override these (e.g., --n_epochs, --learning_rate) if you have special requirements, but defaults are usually fine for a first run.\n",
    "\n",
    "For chemistry tasks, ~4 epochs is typical; for small datasets (<500 examples), you may run a few more epochs to help the model generalize.\n",
    "\n",
    "OpenAI will queue and run the job on their servers ‚Äî training may take minutes to an hour depending on dataset size and model complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OJpcSNP81pzj",
    "outputId": "9d2ac76b-2aab-44cd-d1a4-6811b7c33e3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"ftjob-McgUcNQdqzxhlImTVVgoNWGl\",\n",
      "  \"created_at\": 1759132118,\n",
      "  \"error\": {\n",
      "    \"code\": null,\n",
      "    \"message\": null,\n",
      "    \"param\": null\n",
      "  },\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"finished_at\": null,\n",
      "  \"hyperparameters\": {\n",
      "    \"batch_size\": \"auto\",\n",
      "    \"learning_rate_multiplier\": \"auto\",\n",
      "    \"n_epochs\": \"auto\"\n",
      "  },\n",
      "  \"model\": \"gpt-3.5-turbo-0125\",\n",
      "  \"object\": \"fine_tuning.job\",\n",
      "  \"organization_id\": \"org-vVj7NX8f155Kazao4Phi3tol\",\n",
      "  \"result_files\": [],\n",
      "  \"seed\": 1417228779,\n",
      "  \"status\": \"validating_files\",\n",
      "  \"trained_tokens\": null,\n",
      "  \"training_file\": \"file-FFgRh4gh7pBug9Lsrb7m6e\",\n",
      "  \"validation_file\": null,\n",
      "  \"estimated_finish\": null,\n",
      "  \"integrations\": [],\n",
      "  \"metadata\": null,\n",
      "  \"method\": {\n",
      "    \"type\": \"supervised\",\n",
      "    \"dpo\": null,\n",
      "    \"reinforcement\": null,\n",
      "    \"supervised\": {\n",
      "      \"hyperparameters\": {\n",
      "        \"batch_size\": \"auto\",\n",
      "        \"learning_rate_multiplier\": \"auto\",\n",
      "        \"n_epochs\": \"auto\"\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"user_provided_suffix\": null,\n",
      "  \"usage_metrics\": null,\n",
      "  \"shared_with_openai\": false,\n",
      "  \"eval_id\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!openai api fine_tuning.jobs.create -F \"file-FFgRh4gh7pBug9Lsrb7m6e\" -m gpt-3.5-turbo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsacYNyO1x_k"
   },
   "source": [
    "Step 4: Monitor the Fine-Tune and Wait  \n",
    "\n",
    "You can check the status of your fine-tuning jobs with:  \n",
    "\n",
    "```bash\n",
    "openai api fine_tunes.list\n",
    "```\n",
    "This will list both active and completed fine-tune jobs, showing whether your job is pending, running, or completed.\n",
    "\n",
    "OpenAI typically also sends an email or notification when training finishes.\n",
    "If successful, you‚Äôll receive a new model ID, for example: `ft:gpt-3.5-turbo:your-org:chem-tuned-model:version`\n",
    "\n",
    "\n",
    "This fine-tuned model is now stored on OpenAI‚Äôs side and ready to use in API calls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xXPspxnG10G9",
    "outputId": "22938178-ab62-4327-908a-52e2f441aeed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"data\": [\n",
      "    {\n",
      "      \"id\": \"ftjob-McgUcNQdqzxhlImTVVgoNWGl\",\n",
      "      \"created_at\": 1759132118,\n",
      "      \"error\": {\n",
      "        \"code\": null,\n",
      "        \"message\": null,\n",
      "        \"param\": null\n",
      "      },\n",
      "      \"fine_tuned_model\": null,\n",
      "      \"finished_at\": 1759132488,\n",
      "      \"hyperparameters\": {\n",
      "        \"batch_size\": 1,\n",
      "        \"learning_rate_multiplier\": 2.0,\n",
      "        \"n_epochs\": 10\n",
      "      },\n",
      "      \"model\": \"gpt-3.5-turbo-0125\",\n",
      "      \"object\": \"fine_tuning.job\",\n",
      "      \"organization_id\": \"org-vVj7NX8f155Kazao4Phi3tol\",\n",
      "      \"result_files\": [],\n",
      "      \"seed\": 1417228779,\n",
      "      \"status\": \"running\",\n",
      "      \"trained_tokens\": null,\n",
      "      \"training_file\": \"file-FFgRh4gh7pBug9Lsrb7m6e\",\n",
      "      \"validation_file\": null,\n",
      "      \"estimated_finish\": null,\n",
      "      \"integrations\": [],\n",
      "      \"metadata\": null,\n",
      "      \"method\": {\n",
      "        \"type\": \"supervised\",\n",
      "        \"dpo\": null,\n",
      "        \"reinforcement\": null,\n",
      "        \"supervised\": {\n",
      "          \"hyperparameters\": {\n",
      "            \"batch_size\": 1,\n",
      "            \"learning_rate_multiplier\": 2.0,\n",
      "            \"n_epochs\": 10\n",
      "          }\n",
      "        }\n",
      "      },\n",
      "      \"user_provided_suffix\": null,\n",
      "      \"usage_metrics\": null,\n",
      "      \"shared_with_openai\": false,\n",
      "      \"eval_id\": null\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ftjob-LbUkVBE7LZcwnWwRoBYqqOQa\",\n",
      "      \"created_at\": 1759132098,\n",
      "      \"error\": {\n",
      "        \"code\": null,\n",
      "        \"message\": null,\n",
      "        \"param\": null\n",
      "      },\n",
      "      \"fine_tuned_model\": null,\n",
      "      \"finished_at\": 1759132456,\n",
      "      \"hyperparameters\": {\n",
      "        \"batch_size\": 1,\n",
      "        \"learning_rate_multiplier\": 2.0,\n",
      "        \"n_epochs\": 10\n",
      "      },\n",
      "      \"model\": \"gpt-3.5-turbo-0125\",\n",
      "      \"object\": \"fine_tuning.job\",\n",
      "      \"organization_id\": \"org-vVj7NX8f155Kazao4Phi3tol\",\n",
      "      \"result_files\": [],\n",
      "      \"seed\": 1350736969,\n",
      "      \"status\": \"running\",\n",
      "      \"trained_tokens\": null,\n",
      "      \"training_file\": \"file-FFgRh4gh7pBug9Lsrb7m6e\",\n",
      "      \"validation_file\": null,\n",
      "      \"estimated_finish\": null,\n",
      "      \"integrations\": [],\n",
      "      \"metadata\": null,\n",
      "      \"method\": {\n",
      "        \"type\": \"supervised\",\n",
      "        \"dpo\": null,\n",
      "        \"reinforcement\": null,\n",
      "        \"supervised\": {\n",
      "          \"hyperparameters\": {\n",
      "            \"batch_size\": 1,\n",
      "            \"learning_rate_multiplier\": 2.0,\n",
      "            \"n_epochs\": 10\n",
      "          }\n",
      "        }\n",
      "      },\n",
      "      \"user_provided_suffix\": null,\n",
      "      \"usage_metrics\": null,\n",
      "      \"shared_with_openai\": false,\n",
      "      \"eval_id\": null\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ftjob-2Nxc4vIpewfqoMI9rFmRiXu0\",\n",
      "      \"created_at\": 1759130333,\n",
      "      \"error\": {\n",
      "        \"code\": null,\n",
      "        \"message\": null,\n",
      "        \"param\": null\n",
      "      },\n",
      "      \"fine_tuned_model\": \"ft:gpt-3.5-turbo-0125:washington-university-in-st-louis-zheng-group::CL2YjEu2\",\n",
      "      \"finished_at\": 1759130759,\n",
      "      \"hyperparameters\": {\n",
      "        \"batch_size\": 1,\n",
      "        \"learning_rate_multiplier\": 2.0,\n",
      "        \"n_epochs\": 10\n",
      "      },\n",
      "      \"model\": \"gpt-3.5-turbo-0125\",\n",
      "      \"object\": \"fine_tuning.job\",\n",
      "      \"organization_id\": \"org-vVj7NX8f155Kazao4Phi3tol\",\n",
      "      \"result_files\": [\n",
      "        \"file-MxVVtCQchNwxSz7SKjKA7n\"\n",
      "      ],\n",
      "      \"seed\": 839734511,\n",
      "      \"status\": \"succeeded\",\n",
      "      \"trained_tokens\": 3870,\n",
      "      \"training_file\": \"file-FFgRh4gh7pBug9Lsrb7m6e\",\n",
      "      \"validation_file\": null,\n",
      "      \"estimated_finish\": null,\n",
      "      \"integrations\": [],\n",
      "      \"metadata\": null,\n",
      "      \"method\": {\n",
      "        \"type\": \"supervised\",\n",
      "        \"dpo\": null,\n",
      "        \"reinforcement\": null,\n",
      "        \"supervised\": {\n",
      "          \"hyperparameters\": {\n",
      "            \"batch_size\": 1,\n",
      "            \"learning_rate_multiplier\": 2.0,\n",
      "            \"n_epochs\": 10\n",
      "          }\n",
      "        }\n",
      "      },\n",
      "      \"user_provided_suffix\": null,\n",
      "      \"usage_metrics\": null,\n",
      "      \"shared_with_openai\": false,\n",
      "      \"eval_id\": null\n",
      "    }\n",
      "  ],\n",
      "  \"has_more\": false,\n",
      "  \"object\": \"list\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!openai api fine_tuning.jobs.list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xc_6dO_Y13PG"
   },
   "source": [
    "Step 5: Using the Fine-Tuned Model  \n",
    "Once ready, you use it just like any other model in the API, but by specifying the new model name. For example, in Python:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gyFbIVfR15K_",
    "outputId": "35e3e35e-30a8-47a0-a085-982afacf7a65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C2H5OH\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"ft:gpt-3.5-turbo-0125:washington-university-in-st-louis-zheng-group::CL2YjEu2\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Q: What is the formula of ethanol?\\nA:\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eK8KNVTEZSR-"
   },
   "source": [
    "Step 6: Test and Refine  \n",
    "\n",
    "Evaluate your fine-tuned model on examples not in the training set to check performance. If weaknesses appear (e.g., struggles with organometallic nomenclature), add more high-quality, consistent examples and fine-tune again. Iteration is normal, but avoid over-tuning on inconsistent data. You can also fine-tune in stages by using your fine-tuned model as the new base.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V9TlVDyaZSX7"
   },
   "source": [
    "Step 7: Cost and Considerations  \n",
    "\n",
    "Fine-tuning has costs: you pay for training tokens and a slightly higher per-call rate for the custom model. Weigh the benefits against using a base model with careful prompting. Remember, fine-tuning shapes style and task specialization, not general knowledge‚Äîit won‚Äôt add facts beyond the base model‚Äôs cutoff unless explicitly provided in training data.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LKmZV3BP2AqB"
   },
   "source": [
    "### 4.3 Building a Chemistry-Focused Conversational Agent  \n",
    "\n",
    "LLMs can power interactive chemistry assistants that maintain dialogue, use tools, and serve students or researchers. To improve reliability, combine them with **retrieval-augmented generation (RAG)**, where the model fetches relevant passages (e.g., lab manuals, reaction guides) before answering. This reduces hallucinations and ensures responses are grounded in your own chemistry resources.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFjcrD1GZoNo"
   },
   "source": [
    "Step 1: Prepare and Chunk Your Chemistry Content. Split your chemistry sources into small, meaningful text chunks (e.g., by paragraph). Each chunk should fit under the token limits (typically 4096 or 8192)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-FFh0bs82Iob"
   },
   "outputs": [],
   "source": [
    "# Example documents\n",
    "documents = [\n",
    "    {\n",
    "        \"title\": \"IR Guide\",\n",
    "        \"text\": \"A broad peak around 3200-3600 cm^-1 suggests O‚ÄìH...\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Reactions\",\n",
    "        \"text\": \"To dehydrate alcohols, use H2SO4 or POCl3...\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Break documents into chunks (split on blank lines)\n",
    "chunks = [\n",
    "    {\"text\": para}\n",
    "    for doc in documents\n",
    "    for para in doc[\"text\"].split(\"\\n\\n\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcf-_C098reJ"
   },
   "source": [
    "Step 2: Generate Embeddings with OpenAI. Use `text-embedding-ada-002` to convert each\n",
    "\n",
    "chunk to a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AATYGGK90z45",
    "outputId": "08a1c5ff-adb1-48cf-e80f-0c4114809809"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First embedding dimension: 1536\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the client (make sure OPENAI_API_KEY is set in your environment)\n",
    "client = OpenAI()\n",
    "\n",
    "# Example: assume you already have a list of dicts called 'chunks'\n",
    "# Each chunk looks like: {\"text\": \"some text here\"}\n",
    "texts = [c[\"text\"] for c in chunks]\n",
    "\n",
    "# Create embeddings with the latest API\n",
    "resp = client.embeddings.create(\n",
    "    model=\"text-embedding-ada-002\",  # or \"text-embedding-3-small\"/\"text-embedding-3-large\"\n",
    "    input=texts\n",
    ")\n",
    "\n",
    "# Add embeddings back into chunks\n",
    "for i, emb in enumerate(resp.data):\n",
    "    chunks[i][\"embedding\"] = emb.embedding\n",
    "\n",
    "# Optional: check the first embedding length\n",
    "print(\"First embedding dimension:\", len(chunks[0][\"embedding\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7ZBCvhs8vJ7"
   },
   "source": [
    "Step 3: Embed the Question and Retrieve Similar Chunks. Embed the user's question, then use cosine similarity to find top-matching chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nzjd-Clv02GG",
    "outputId": "5ae181f5-e7f0-42c4-b71b-4a51590da25b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most relevant context:\n",
      " A broad peak around 3200-3600 cm^-1 suggests O‚ÄìH...\n",
      "\n",
      "To dehydrate alcohols, use H2SO4 or POCl3...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize client (requires OPENAI_API_KEY in your environment)\n",
    "client = OpenAI()\n",
    "\n",
    "# Example query\n",
    "query = \"What does a strong IR peak at 1700 mean?\"\n",
    "\n",
    "# Get embedding for the query\n",
    "qresp = client.embeddings.create(\n",
    "    input=[query],\n",
    "    model=\"text-embedding-ada-002\"  # or \"text-embedding-3-small\"/\"text-embedding-3-large\"\n",
    ")\n",
    "qvec = qresp.data[0].embedding\n",
    "\n",
    "# Define cosine similarity\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (norm(a) * norm(b))\n",
    "\n",
    "# Rank chunks by similarity\n",
    "scores = [(cosine_similarity(qvec, c[\"embedding\"]), c[\"text\"]) for c in chunks]\n",
    "\n",
    "# Take top 2 most relevant chunks\n",
    "top_chunks = sorted(scores, key=lambda x: x[0], reverse=True)[:2]\n",
    "context = \"\\n\\n\".join([text for _, text in top_chunks])\n",
    "\n",
    "print(\"Most relevant context:\\n\", context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72qJKKBN8ysu"
   },
   "source": [
    "Step 4: Create Prompt and Call ChatGPT. Construct a prompt that includes the context, then pass it to ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LQT4pPmj8zy9",
    "outputId": "e9cde570-ce1a-415c-c7fb-85c22c8ec509"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A strong IR peak at 1700 cm^-1 typically indicates the presence of a carbonyl group (C=O) in the molecule.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize client\n",
    "client = OpenAI()\n",
    "\n",
    "# Build the prompt with retrieved context\n",
    "prompt = f\"\"\"\n",
    "You are a chemistry assistant. Use the context below to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Get completion\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Print the assistant's answer\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzibviR-8-ks"
   },
   "source": [
    "### 4.5 Tool Use and Function-Calling  \n",
    "\n",
    "LLMs become far more powerful when connected to external tools‚Äîfunctions, libraries, or databases‚Äîthat provide precise results. OpenAI‚Äôs **function calling** feature lets models output structured calls instead of guessing, enabling verifiable answers (e.g., chemical properties from RDKit or PubChem).  \n",
    "\n",
    "For example, when asked *‚ÄúWhat is the molecular weight of caffeine?‚Äù*, the model can generate a JSON call like `get_molecular_weight(\"caffeine\")`. Your system executes this function, retrieves the result, and returns it to the model, which then produces the final answer.  \n",
    "\n",
    "This approach transforms a chatbot from speculation to computation, making it a reliable assistant for chemistry.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HOO-0do6890H",
    "outputId": "04aa7ddc-b192-4a13-8427-1b5dad2c2a9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The molecular weight of caffeine is approximately 194.19 g/mol.\n"
     ]
    }
   ],
   "source": [
    "def get_molecular_weight(compound_name):\n",
    "    weights = {\n",
    "        \"water\": 18.015,\n",
    "        \"caffeine\": 194.19\n",
    "    }\n",
    "    return weights.get(compound_name.lower(), \"Unknown\")\n",
    "\n",
    "\n",
    "# Example function call\n",
    "compound = \"caffeine\"\n",
    "mw = get_molecular_weight(compound)\n",
    "\n",
    "if mw != \"Unknown\":\n",
    "    print(f\"The molecular weight of {compound} is approximately {mw} g/mol.\")\n",
    "else:\n",
    "    print(f\"Molecular weight for {compound} is not in the database.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gF-B26TM9Lx-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}